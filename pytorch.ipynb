{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the raw data using the starter code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, lowercase=True):\n",
    "    sents = []\n",
    "    tags = []\n",
    "    with open(path, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f.read().splitlines():\n",
    "            sent = []\n",
    "            tag = []\n",
    "            for pair in line.split('####')[1].split(' '):\n",
    "                tn, tg = pair.rsplit('=', 1)\n",
    "                if lowercase:\n",
    "                    sent.append(tn.lower())\n",
    "                else:\n",
    "                    sent.append(tn)\n",
    "                tag.append(tg)\n",
    "            sents.append(sent)\n",
    "            tags.append(tag)\n",
    "    return sents, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, train_tags = load_data(\"data/twitter1_train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the raw tweets and tags, we have to clean the data and do a few preprocessing steps before we can start training. PyTorch requires us to label all tokens using positive integers, so in this step we will build a hashmap to help us convert raw sequences of alphanumeric tokens to sequences of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_idx(train):\n",
    "    tokens = set()\n",
    "    for sent in train:\n",
    "        tokens.update(sent)\n",
    "    tokens = sorted(list(tokens))\n",
    "    vocab2idx = dict(zip(tokens, range(1, len(tokens)+1)))\n",
    "    vocab2idx[\"<PAD>\"] = 0\n",
    "    return vocab2idx\n",
    "\n",
    "def convert_to_idx(sents, word2idx):\n",
    "    for sent in sents:\n",
    "        for i in range(len(sent)):\n",
    "            sent[i] = word2idx[sent[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idx = get_vocab_idx(train_sents)\n",
    "convert_to_idx(train_sents, vocab2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to convert the tags into integers that can then be fed to PyTorch's categorical cross entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {\"<PAD>\": 0, \"O\": 1, \"T-NEG\": 2, \"T-NEU\": 3, \"T-POS\": 4}\n",
    "convert_to_idx(train_tags, tag2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any computation model in PyTorch, from a single layer to an entire model, inherits the base class nn.Module. To build our own model, we will need to construct our own class that inherits from nn.Module as well. Other than the constructor function, we will need to fill out the `forward()` function, which takes in the data and returns the output predictions, and will be implicitly called when we feed our data to an instance of our model. \n",
    "\n",
    "WARNING: this example below is only to help you understand how to use PyTorch, and is NOT what you will be doing for this homework! It predicts the current label based on only the contextualized representation vector of the current token. In your model, you will be predicting the current label based on the previous label as well as the vector representation of the current token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeSentence(nn.Module):\n",
    "    def __init__(self, word_dim, num_words, num_tags):\n",
    "        super(EncodeSentence, self).__init__()\n",
    "        self.word_emb = nn.Embedding(num_words, word_dim, padding_idx=0)\n",
    "        # Output dimension is word_dim//2 because bidirectional doubles the output dimension\n",
    "        self.contextualizer = nn.LSTM(word_dim, word_dim//2, batch_first=True, bidirectional=True)\n",
    "        self.output = nn.Linear(word_dim, num_tags)\n",
    "        \n",
    "    def forward(self, sent):\n",
    "        sent_embed = self.word_emb(sent)\n",
    "        sent_embed, _ = self.contextualizer(sent_embed)\n",
    "        output_scores = self.output(sent_embed)\n",
    "        return F.log_softmax(output_scores, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch cannot handle jagged arrays, so if we want to process our input data in batches (with sentences in a batch having different lengths), we have to either pad or trim all sentences to the same size. In this example, I'm padding all sentences in a batch of size 10 so that they all have the same length as the longest sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sents(sents, pad_idx=0):\n",
    "    padded_sents = []\n",
    "    maxlen = max([len(sent) for sent in sents])\n",
    "    for sent in sents:\n",
    "        padded_sent = sent.copy()\n",
    "        padded_sent.extend([pad_idx]*(maxlen-len(sent)))\n",
    "        padded_sents.append(padded_sent)\n",
    "    return padded_sents\n",
    "\n",
    "train_batch = pad_sents(train_sents[:10])\n",
    "train_label = pad_sents(train_tags[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an instance of our model, and feed our batch to it to get prediction scores for each label type for each token for all sentences in our batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncodeSentence(256, len(vocab2idx), len(tag2idx))\n",
    "scores = model(torch.tensor(train_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize our model parameters, we need to create an instance of any optimizer algorithms provided by PyTorch. Here we're using ADAM. `model.parameters()` will return all the parameters of our model, which will be used by our optimizer instance. If you have more than one model, and you want to optimize all of them end-to-end, then do not forget to feed all of them to the same optimizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss_sum` allows us to accumulate the gradient of all time steps up to `maxlen`, which is the length of the longest sequence in the training batch. This step is necessary because `F.nll_loss()` doesn't allow us to calculate the loss of all time steps simultaneously. If you manage to find a loss function provided by PyTorch that actually calculate categorical softmax loss of the entire sequence in one call, feel free to use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = scores.size(1)\n",
    "loss_sum = torch.tensor([0.],)\n",
    "for i in range(maxlen):\n",
    "    loss_sum += F.nll_loss(scores[:, i, :], torch.tensor(train_label)[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've accumulated the loss at all time steps, backpropagate it through the model to calculate gradient for all parameters, and call `optimizer.step()` to perform a single parameter update througout the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sum.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
